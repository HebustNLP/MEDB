import pandas as pd
import time
from tqdm import tqdm
import os
from openai import AzureOpenAI

# ==================== é…ç½®åŒº ====================
ORIGINAL_EXCEL = '/root/lsz/CGM/code/é£Ÿç‰©è¯_BERTopic.xlsx'
EXISTING_CSV = '/root/lsz/CGM/code/é£Ÿç‰©ç±»åˆ«åˆ†ç±»ç»“æœ.csv'
API_ENDPOINT = 'https://ai-eval-found.openai.azure.com/'
API_KEY = 'YOUR_API_KEY_HERE'  # â—æ›¿æ¢ä¸ºçœŸå®API Key
DEPLOYMENT_NAME = 'gpt-4.1'
API_VERSION = '2024-08-01-preview'
GROUP_SIZE = 50
REQUIRED_COLUMNS = ['index', 'é£Ÿç‰©è¯', 'é¢‘æ¬¡', 'ç±»åˆ«']  # ä¸¥æ ¼å››åˆ—é¡ºåº
# ===============================================

# ========== 1. åˆå§‹åŒ–å®¢æˆ·ç«¯ ==========
client = AzureOpenAI(
    azure_endpoint=API_ENDPOINT,
    api_key=API_KEY,
    api_version=API_VERSION,
)

# ========== 2. é‡å»ºåŸå§‹æ•°æ®ç´¢å¼•ï¼ˆä¸¥æ ¼å››åˆ—ç»“æ„ï¼‰==========
print("ğŸ” æ­¥éª¤1/4: é‡å»ºåŸå§‹æ•°æ®ç´¢å¼•...")
original_df = pd.read_excel(ORIGINAL_EXCEL)
if not {'é£Ÿç‰©è¯', 'å…¨å±€é¢‘æ¬¡'}.issubset(original_df.columns):
    raise ValueError(f"åŸå§‹Excelç¼ºå¤±å¿…è¦åˆ—ï¼å¯ç”¨åˆ—: {list(original_df.columns)}")

original_df = original_df[['é£Ÿç‰©è¯', 'å…¨å±€é¢‘æ¬¡']].copy()
original_df['index'] = range(len(original_df))
original_df = original_df[['index', 'é£Ÿç‰©è¯', 'å…¨å±€é¢‘æ¬¡']]
original_df.columns = ['index', 'é£Ÿç‰©è¯', 'é¢‘æ¬¡']
print(f"âœ… åŸå§‹æ•°æ®é‡å»ºå®Œæˆ | æ€»è¯æ•°: {len(original_df)}")

# ========== 3. éªŒè¯ç°æœ‰CSVç»“æ„ ==========
if not os.path.exists(EXISTING_CSV):
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åˆ†ç±»ç»“æœæ–‡ä»¶: {EXISTING_CSV}")

existing_df = pd.read_csv(EXISTING_CSV, encoding='utf-8')
missing_cols = [col for col in REQUIRED_COLUMNS if col not in existing_df.columns]
if missing_cols:
    raise ValueError(f"âŒ CSVç¼ºå¤±å¿…è¦åˆ—: {missing_cols} | å½“å‰åˆ—: {list(existing_df.columns)}")

existing_indices = set(existing_df['index'].unique())
all_indices = set(original_df['index'])
missing_indices = sorted(all_indices - existing_indices)

print(f"ğŸ“Š ç´¢å¼•æ£€æŸ¥:")
print(f"   â€¢ åŸå§‹æ€»è¯æ•°: {len(all_indices)}")
print(f"   â€¢ å·²åˆ†ç±»: {len(existing_indices)} | ç¼ºå¤±: {len(missing_indices)}")
if missing_indices:
    print(f"   â€¢ ç¼ºå¤±ç´¢å¼•ç¤ºä¾‹: {missing_indices[:10]}...")

if not missing_indices:
    print("\nğŸ‰ æ‰€æœ‰æ•°æ®å·²å®Œæ•´ï¼æ‰§è¡Œæœ€ç»ˆæ•´ç†...")
    final_df = existing_df[REQUIRED_COLUMNS].copy()
    final_df = final_df.drop_duplicates(subset=['index'], keep='first').sort_values('index').reset_index(drop=True)
    final_df.to_csv(EXISTING_CSV, index=False, encoding='utf-8')
    
    # ç”Ÿæˆç»Ÿè®¡
    stats = final_df['ç±»åˆ«'].value_counts().reset_index()
    stats.columns = ['ç±»åˆ«', 'è¯æ•°']
    stats.to_csv(EXISTING_CSV.replace('.csv', '_ç±»åˆ«ç»Ÿè®¡.csv'), index=False, encoding='utf-8')
    print(f"âœ… æ–‡ä»¶å·²æ ‡å‡†åŒ– | ç»Ÿè®¡æŠ¥è¡¨å·²æ›´æ–°")
    exit(0)

# ========== 4. æå–ç¼ºå¤±æ•°æ® ==========
missing_df = original_df[original_df['index'].isin(missing_indices)].copy()
word_tuples = [(row['index'], row['é£Ÿç‰©è¯'], row['é¢‘æ¬¡']) for _, row in missing_df.iterrows()]
word_groups = [word_tuples[i:i+GROUP_SIZE] for i in range(0, len(word_tuples), GROUP_SIZE)]
print(f"\nğŸ§  æ­¥éª¤2/4: è¡¥å…¨ {len(missing_indices)} ä¸ªç¼ºå¤±é¡¹ | åˆ† {len(word_groups)} ç»„")

# ========== 5. ç²¾å‡†è¡¥å…¨ï¼ˆä¸¥æ ¼å››åˆ—è¿½åŠ ï¼‰==========
print("\nğŸ¤– æ­¥éª¤3/4: å¼€å§‹è¡¥å…¨æµç¨‹...")
appended_count = 0

for group_idx, group in enumerate(tqdm(word_groups, desc="è¡¥å…¨æµç¨‹")):
    words_text = "\n".join([f"{idx}. {word}" for idx, word, _ in group])
    prompt = (
        f"åˆ†ç±»è¦æ±‚ï¼š8ç±»åˆ«ï¼ˆä¸»é£Ÿæˆ–è€…æ‚ç²®,è±†ç±»åŠåˆ¶å“,æœæ±ä¸é¥®æ–™,ä¹³ç±»åŠåˆ¶å“,è”¬èœç±»,è‚‰ç±»,æ°´æœç±»,å…¶å®ƒï¼‰\n"
        f"è¿”å›æ ¼å¼ï¼šç´¢å¼•,è¯,ç±»åˆ«ï¼ˆä¸¥æ ¼ä¸‰å­—æ®µï¼Œé€—å·åˆ†éš”ï¼‰\n"
        f"ç¤ºä¾‹ï¼š1,ç±³é¥­,ä¸»é£Ÿæˆ–è€…æ‚ç²®\n\n"
        f"å¾…åˆ†ç±»è¯ï¼š\n{words_text}"
    )
    
    retry = 0
    while retry < 3:
        try:
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,
                messages=[
                    {"role": "system", "content": "ä¸“ä¸šé£Ÿç‰©åˆ†ç±»åŠ©æ‰‹ï¼Œè¿”å›æ ¼å¼ï¼šç´¢å¼•,è¯,ç±»åˆ«"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2500,
                temperature=0.0
            )
            
            # ä¸¥æ ¼æŒ‰å››åˆ—é¡ºåºæ„å»ºæ–°è¡Œ
            for line in response.choices[0].message.content.strip().split('\n'):
                parts = [p.strip() for p in line.split(',', 2)]
                if len(parts) < 3: 
                    continue
                
                try:
                    idx = int(parts[0])
                    word = parts[1]
                    category = parts[2]
                    
                    # éªŒè¯ç±»åˆ«æœ‰æ•ˆæ€§
                    valid_cats = ["ä¸»é£Ÿæˆ–è€…æ‚ç²®","è±†ç±»åŠåˆ¶å“","æœæ±ä¸é¥®æ–™","ä¹³ç±»åŠåˆ¶å“","è”¬èœç±»","è‚‰ç±»","æ°´æœç±»","å…¶å®ƒ"]
                    if not any(cat in category for cat in valid_cats):
                        continue
                    
                    # è·å–åŸå§‹é¢‘æ¬¡
                    orig = next((r for r in group if r[0] == idx), None)
                    if not orig:
                        continue
                    
                    # âœ… å…³é”®ä¿®æ­£ï¼šä¸¥æ ¼æŒ‰å››åˆ—é¡ºåºæ„å»ºDataFrame
                    new_row = pd.DataFrame([{
                        'index': idx,
                        'é£Ÿç‰©è¯': word,
                        'é¢‘æ¬¡': orig[2],
                        'ç±»åˆ«': category
                    }], columns=REQUIRED_COLUMNS)  # å¼ºåˆ¶åˆ—é¡ºåº
                    
                    # å®æ—¶è¿½åŠ ï¼ˆæ— è¡¨å¤´ï¼‰
                    new_row.to_csv(EXISTING_CSV, mode='a', header=False, index=False, encoding='utf-8')
                    appended_count += 1
                    
                except Exception:
                    continue
            
            time.sleep(1.0)
            break
        except Exception as e:
            retry += 1
            if retry == 3:
                print(f"\nâš ï¸ ç»„ {group_idx+1} å¤±è´¥: {str(e)[:80]}")
            else:
                time.sleep(3)

print(f"\nâœ… è¡¥å…¨æµç¨‹å®Œæˆ | æ–°å¢ {appended_count} æ¡è®°å½•")

# ========== 6. æœ€ç»ˆæ ‡å‡†åŒ–ï¼ˆå¼ºåˆ¶å››åˆ—é¡ºåºï¼‰==========
print("\nğŸ’¾ æ­¥éª¤4/4: æ ‡å‡†åŒ–æœ€ç»ˆæ–‡ä»¶...")
final_df = pd.read_csv(EXISTING_CSV, encoding='utf-8')

# ä¸¥æ ¼æ ¡éªŒå¹¶é‡æ’å››åˆ—
if not all(col in final_df.columns for col in REQUIRED_COLUMNS):
    raise ValueError(f"CSVåˆ—ç»“æ„å¼‚å¸¸ï¼å½“å‰åˆ—: {list(final_df.columns)}")
final_df = final_df[REQUIRED_COLUMNS].copy()

# å»é‡ + æŒ‰indexæ’åº
final_df = final_df.drop_duplicates(subset=['index'], keep='first').sort_values('index').reset_index(drop=True)
final_df.to_csv(EXISTING_CSV, index=False, encoding='utf-8')

# ç”Ÿæˆç»Ÿè®¡
stats = final_df['ç±»åˆ«'].value_counts().reset_index()
stats.columns = ['ç±»åˆ«', 'è¯æ•°']
stats_file = EXISTING_CSV.replace('.csv', '_ç±»åˆ«ç»Ÿè®¡.csv')
stats.to_csv(stats_file, index=False, encoding='utf-8')

# éªŒè¯å®Œæ•´æ€§
final_indices = set(final_df['index'])
missing_after = sorted(all_indices - final_indices)
print(f"\nğŸ” å®Œæ•´æ€§éªŒè¯:")
print(f"   â€¢ æœ€ç»ˆæ€»è¯æ•°: {len(final_df)}")
print(f"   â€¢ ä»ç¼ºå¤±: {len(missing_after)}")
if missing_after:
    print(f"   â€¢ æœªè¡¥å…¨ç´¢å¼•: {missing_after[:15]}...")
    print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥APIè¿”å›æˆ–æ‰‹åŠ¨è¡¥å……å‰©ä½™é¡¹")
else:
    print("ğŸ‰ æ‰€æœ‰æ•°æ®å·²å®Œæ•´åˆ†ç±»ï¼")

print(f"\nâœ… æ ‡å‡†åŒ–æ–‡ä»¶: {EXISTING_CSV} (åˆ—é¡ºåº: index â†’ é£Ÿç‰©è¯ â†’ é¢‘æ¬¡ â†’ ç±»åˆ«)")
print(f"âœ… ç»Ÿè®¡æŠ¥è¡¨: {stats_file}")
print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:\n{stats.to_string(index=False)}")